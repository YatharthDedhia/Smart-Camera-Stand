{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Person Detection using a MobileNetV2 Model"
      ],
      "metadata": {
        "id": "f_vWEO3hEMp_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prerequisite\n",
        "Download the dataset from [here](https://github.com/YatharthDedhia/Eklavya-Smart-Stand.git) and upload the Dataset to your drive. Mount your drive in your colab notebook."
      ],
      "metadata": {
        "id": "4e1pyfAtERZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_model_optimization\n"
      ],
      "metadata": {
        "id": "bXxhW7h6f-gO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pathlib\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as tfl\n",
        "import tempfile\n",
        "import sys\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "id": "e5C8WpBPEPjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CgPZLw7op9gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Testing sets \n",
        "We are splitting the dataset into a testing dataset and a validation dataset."
      ],
      "metadata": {
        "id": "FmdFN6c1e1Tz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (224, 224)\n",
        "directory = \"/content/drive/MyDrive/dataset\"\n",
        "train_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='training',\n",
        "                                             seed=42)\n",
        "test_dataset = image_dataset_from_directory(directory,\n",
        "                                             shuffle=True,\n",
        "                                             image_size=IMG_SIZE,\n",
        "                                             validation_split=0.2,\n",
        "                                             subset='validation',\n",
        "                                             seed=42)"
      ],
      "metadata": {
        "id": "17W7x6Iz5OaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)"
      ],
      "metadata": {
        "id": "jkzZ1GGO5SFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "The images a rotated so the model is trained better."
      ],
      "metadata": {
        "id": "2r3yZ7--jB7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_augmenter():\n",
        "    '''\n",
        "    Create a Sequential model composed of 2 layers\n",
        "    Returns:\n",
        "        tf.keras.Sequential\n",
        "    '''\n",
        "    ### START CODE HERE\n",
        "    data_augmentation = tf.keras.Sequential()\n",
        "    data_augmentation.add(RandomFlip(\"horizontal\"))\n",
        "    data_augmentation.add(RandomRotation(0.2))\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return data_augmentation"
      ],
      "metadata": {
        "id": "RsNNMpsC5T1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation = data_augmenter()\n",
        "data_augmentation = data_augmenter()\n",
        "for image, _ in train_dataset.take(1):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    first_image = image[0]\n",
        "    for i in range(9):\n",
        "        ax = plt.subplot(3, 3, i + 1)\n",
        "        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
        "        plt.imshow(augmented_image[0] / 255)\n",
        "        plt.axis('off')"
      ],
      "metadata": {
        "id": "K82VvfNh5VZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the dataset"
      ],
      "metadata": {
        "id": "560-CPUajMCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
      ],
      "metadata": {
        "id": "45kicNnfBxPu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instantiating the MobileNetV2 architecture"
      ],
      "metadata": {
        "id": "TnjNPVcesVb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = (224,224)\n",
        "IMG_SHAPE = IMG_SIZE + (3,)\n",
        "base_model = tf.keras.applications.mobilenet_v2.MobileNetV2(include_top=False,weights='imagenet')\n",
        "base_model.summary()\n",
        "type(base_model)"
      ],
      "metadata": {
        "id": "R0D-Tt5qB9HT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruning the model"
      ],
      "metadata": {
        "id": "iYrUbJZ8uH8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_model_optimization as tfmot\n",
        "\n",
        "prune_low_magnitude = tfmot.sparsity.keras.prune_low_magnitude\n",
        "\n",
        "# Compute end step to finish pruning after 2 epochs.\n",
        "batch_size = 32\n",
        "epochs = 2\n",
        "validation_split = 0.1 # 10% of training set will be used for validation set. \n",
        "\n",
        "num_images = 6000\n",
        "model_for_pruning = prune_low_magnitude(base_model)\n",
        "\n",
        "# # `prune_low_magnitude` requires a recompile.\n",
        "model_for_pruning.compile(optimizer='adam',\n",
        "                loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "model_for_pruning.summary()"
      ],
      "metadata": {
        "id": "cymqTokc3CUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing the model\n",
        "The layers of the model are freezed in transfer learning. Dropout is also applied to ignore certain neurons while training."
      ],
      "metadata": {
        "id": "noinlVVOiqRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def people_model(base_model,image_shape=IMG_SIZE, data_augmentation=data_augmenter()):\n",
        "    ''' Define a tf.keras model for binary classification out of the MobileNetV2 model\n",
        "    Arguments:\n",
        "        image_shape -- Image width and height\n",
        "        data_augmentation -- data augmentation function\n",
        "    Returns:\n",
        "    Returns:\n",
        "        tf.keras.model\n",
        "    '''\n",
        "    \n",
        "    \n",
        "    input_shape = image_shape + (3,)\n",
        "    \n",
        "    # freeze the base model by making it non trainable\n",
        "    base_model.trainable = False\n",
        "\n",
        "    # create the input layer (Same as the imageNetv2 input size)\n",
        "    inputs = tf.keras.Input(shape=input_shape) \n",
        "    \n",
        "    # apply data augmentation to the inputs\n",
        "    x = data_augmentation(inputs)\n",
        "    \n",
        "    # data preprocessing using the same weights the model was trained on\n",
        "    x = preprocess_input(x) \n",
        "    \n",
        "    # set training to False to avoid keeping track of statistics in the batch norm layer\n",
        "    x = base_model(x, training=False) \n",
        "    \n",
        "    # add the new Binary classification layers\n",
        "    # use global avg pooling to summarize the info in each channel\n",
        "    x = tfl.GlobalAveragePooling2D()(x)  \n",
        "    # include dropout with probability of 0.2 to avoid overfitting\n",
        "    x = tfl.Dropout(0.2)(x)\n",
        "        \n",
        "    # use a prediction layer with one neuron (as a binary classifier only needs one)\n",
        "    outputs = tfl.Dense(1)(x) \n",
        "    \n",
        "    ###Â END CODE HERE\n",
        "    \n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "BZ_bIw0F0P6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = people_model(base_model,IMG_SIZE, data_augmentation)"
      ],
      "metadata": {
        "id": "ign08BDX3JFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.003\n",
        "model2.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "en_DI9ar3LW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2.summary()"
      ],
      "metadata": {
        "id": "J54T5zC13NNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model"
      ],
      "metadata": {
        "id": "OfWHIuxW41J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_epochs = 10\n",
        "history = model2.fit(train_dataset, validation_data=test_dataset, epochs=initial_epochs)"
      ],
      "metadata": {
        "id": "4bo78VAu3O_z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc = [0.] + history.history['accuracy']\n",
        "val_acc = [0.] + history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(acc, label='Training Accuracy')\n",
        "plt.plot(val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim([min(plt.ylim()),1])\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.ylabel('Cross Entropy')\n",
        "plt.ylim([0,1.0])\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_t6Lyx7T3Tlj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exporting the model\n",
        "The original model is restored with the sparsed weights."
      ],
      "metadata": {
        "id": "bGCabmPJBPG2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_for_export = tfmot.sparsity.keras.strip_pruning(model2)\n",
        "\n",
        "_, pruned_keras_file = tempfile.mkstemp('.h5')\n",
        "tf.keras.models.save_model(model_for_export, \"/content/Saved_Model\", include_optimizer=False)"
      ],
      "metadata": {
        "id": "gJ9BM7Oh3V8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Applying Quantization\n",
        "The model is first converted to a .tflite model. Then, dynamic range pruning method is used to reduce the size of the model."
      ],
      "metadata": {
        "id": "QFb75pCUDxmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_for_export)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "quantized_and_pruned_tflite_model = converter.convert()\n",
        "\n",
        "_, quantized_tflite_file = tempfile.mkstemp('.tflite')\n",
        "\n",
        "with open(\"/content/Saved_Model.tflite\", 'wb') as f:\n",
        "  f.write(quantized_and_pruned_tflite_model)\n",
        "print(\"Size of tflite model \",sys.getsizeof(quantized_and_pruned_tflite_model))"
      ],
      "metadata": {
        "id": "2xiIsM_K3a6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting the model to a C array\n",
        "As we have to use our model on ESP32, converting it into a C array is essential ."
      ],
      "metadata": {
        "id": "r62ZRTmCBUM8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update && apt-get -qq install xxd\n",
        "!xxd -i \"/content/Saved_Model.tflite\" > model_data.cc"
      ],
      "metadata": {
        "id": "C4_tosah3RLd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}